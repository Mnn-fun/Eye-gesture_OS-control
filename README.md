# Eye-gesture_OS-control
A vision-based hands-free computer interaction system that enables users to control cursor and actions using eye movement and facial gestures, designed for accessibility and inclusive computing. 

<h3> Problem </h3>
Most computer systems still depend on a mouse and keyboard, making digital access difficult or impossible for users with motor disabilities. Existing assistive solutions are often expensive, hardware-dependent, or unreliable.

<h3> Solution </h3>
This project introduces a hands-free computer interaction system that enables users to control a computer using eye movement and facial gestures, powered by computer vision and AI. The system works with a standard webcam and requires no specialized hardware.

<h3> Key Innovation </h3>
The core challenge with eye tracking is intent detection â€” people naturally look around.
Our system solves this by combining:
1.Gaze stability and dwell time
2.Controlled facial gestures
3.Intelligent filtering of involuntary movements
This ensures the cursor moves only when the user intends to interact, not while casually viewing the screen.

<h3> How It Works </h3>
Live camera input is processed in real time to detect facial landmarks and eye direction.
These signals pass through an intent-aware decision layer, which maps deliberate actions to system-level controls such as cursor movement and selection, with continuous visual feedback.

<h3> Why It Matters </h3>
Fully hands-free and accessible
Uses affordable, widely available hardware
Designed for real-world usability
Scalable for education, healthcare, and assistive tech

<h3> Impact </h3>
The solution promotes digital inclusion by enabling independent computer access for individuals with limited motor mobility, aligning with inclusive and human-centered innovation goals.

<h3> Video Link </h3>
https://drive.google.com/file/d/1E0JfCZIIpsXfFpBYyLNGucmxuAj7j4Do/view?usp=drive_link


