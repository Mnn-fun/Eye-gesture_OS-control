# Eye-gesture_OS-control
A vision-based hands-free computer interaction system that enables users to control cursor and actions using eye movement and facial gestures, designed for accessibility and inclusive computing. 

# Problem
Most computer systems still depend on a mouse and keyboard, making digital access difficult or impossible for users with motor disabilities. Existing assistive solutions are often expensive, hardware-dependent, or unreliable.

# Solution
This project introduces a hands-free computer interaction system that enables users to control a computer using eye movement and facial gestures, powered by computer vision and AI. The system works with a standard webcam and requires no specialized hardware.

# Key Innovation
The core challenge with eye tracking is intent detection â€” people naturally look around.
Our system solves this by combining:
1.Gaze stability and dwell time
2.Controlled facial gestures
3.Intelligent filtering of involuntary movements
This ensures the cursor moves only when the user intends to interact, not while casually viewing the screen.

# How It Works
Live camera input is processed in real time to detect facial landmarks and eye direction.
These signals pass through an intent-aware decision layer, which maps deliberate actions to system-level controls such as cursor movement and selection, with continuous visual feedback.

# Why It Matters
Fully hands-free and accessible
Uses affordable, widely available hardware
Designed for real-world usability
Scalable for education, healthcare, and assistive tech

# Impact
The solution promotes digital inclusion by enabling independent computer access for individuals with limited motor mobility, aligning with inclusive and human-centered innovation goals.
